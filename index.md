---
layout: default
description: ''
---

## Introduction

<p style="text-align:justify;">
Recent advances at the intersection of natural language processing and computer vision have
made incredible progress, from being able to generate natural language descriptions of images
and videos, to answering questions about them, to even holding free-form conversations
about visual content! The challenge now is to extend this progress to embodied agents that
take actions and interact with their visual environments.
</p>

<p style="text-align:justify;">
This tutorial will provide a comprehensive yet accessible introduction to the key
innovations that have driven progress in language and vision modeling (such as
multi-modal pooling, visual and co-attention, dynamic network composition,
methods for incorporating external knowledge and cooperative/adversarial games).
We will then discuss some of the challenges in building models for tasks that combine
language, vision, and actions, and discuss recently-released interactive 3D
environments that can be used for these (such as House3D, HoME,
MINOS, Matterport3D Simulator, Gibson, Thor &amp; Chalet).
</p>

{% include tasks.html %}

## Materials

Slides will be posted here immediately after the tutorial.

## Presenters

<table class='organizer-pics-four'>
    <tr>
        <td>
        <img width="200" class='im-speaker-pic' src='images/peter.jpg' alt='peter'>
        </td>
        <td>
        <img width="200" class='im-speaker-pic' src='images/abhishek.jpg' alt='abhishek'>
        </td>
        <td>
        <img width="200" class='im-speaker-pic' src='images/qi.jpg' alt='qi'>
        </td>
    </tr>
    <tr>
        <td><a href='http://www.panderson.me/'>Peter Anderson</a> <br>
        Australian National University</td>

        <td><a href='https://abhishekdas.com'>Abhishek Das</a> <br>
        Georgia Tech </td>

        <td> <a href='http://www.qi-wu.me/'>Qi Wu</a> <br>
        The University of Adelaide</td>
    </tr>
</table>
